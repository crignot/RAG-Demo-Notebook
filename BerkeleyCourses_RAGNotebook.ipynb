{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cf10828",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To install all dependencies\n",
    "%pip install -q --upgrade \\\n",
    "    pinecone-client \\\n",
    "    langchain-pinecone \\\n",
    "    langchain-text-splitters \\\n",
    "    langchain \\\n",
    "    google-genai \\\n",
    "    firecrawl-py\n",
    "\n",
    "print(\"Packages installed successfully\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e8eff1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "\n",
    "from google import genai\n",
    "from firecrawl import FirecrawlApp\n",
    "\n",
    "from langchain_core.documents import Document\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_pinecone import PineconeVectorStore, PineconeEmbeddings\n",
    "from pinecone import Pinecone, ServerlessSpec\n",
    "\n",
    "print(\"Imports completed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1315d83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add API keys (accessed via Google Gemini, Pinecome, and Firecrawl)\n",
    "\n",
    "GOOGLE_API_KEY = os.getenv(\"GOOGLE_API_KEY\")\n",
    "PINECONE_API_KEY = os.getenv(\"PINECONE_API_KEY\")\n",
    "FIRECRAWL_API_KEY = os.getenv(\"FIRECRAWL_API_KEY\")\n",
    "#removed to not hardcode keys, find via web source\n",
    "\n",
    "# Set environment variables\n",
    "os.environ[\"GOOGLE_API_KEY\"] = GOOGLE_API_KEY\n",
    "os.environ[\"PINECONE_API_KEY\"] = PINECONE_API_KEY\n",
    "os.environ[\"FIRECRAWL_API_KEY\"] = FIRECRAWL_API_KEY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63f2ed1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scraping course content with Firecrawl\n",
    "\n",
    "COURSE_PAGES = [\n",
    "    \"https://fa25.datastructur.es/\",\n",
    "    \"https://fa25.datastructur.es/policies/exams/\",\n",
    "    \"https://fa25.datastructur.es/policies/extensions/\",\n",
    "    \"https://fa25.datastructur.es/policies/grading/\",\n",
    "    \"https://cs61b-2.gitbook.io/cs61b-textbook/4.-sllists\",\n",
    "    \"https://cs61b-2.gitbook.io/cs61b-textbook/5.-dllists\",\n",
    "]\n",
    "\n",
    "# Initialize Firecrawl\n",
    "app = FirecrawlApp(api_key=os.environ[\"FIRECRAWL_API_KEY\"])\n",
    "\n",
    "scraped_data = []\n",
    "\n",
    "#Scrape course pages\n",
    "for page in COURSE_PAGES:\n",
    "    full_url = page\n",
    "    print(f\"Scraping: {full_url}\")\n",
    "    try:\n",
    "        result = app.scrape(full_url)\n",
    "        text = result.markdown\n",
    "        if len(text) > 100:\n",
    "            scraped_data.append((full_url, text))\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "\n",
    "print(f\"Successfully scraped {len(scraped_data)} pages\")\n",
    "if scraped_data:\n",
    "    print(f\"Total characters: {sum(len(text) for _, text in scraped_data):,}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78a74c1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create LangChain documents\n",
    "documents = []\n",
    "\n",
    "#Looping through scraped_data (list of tuples: url, text) and create Document objects\n",
    "for url, text in scraped_data:\n",
    "    print(f\"Converting: {url}\")\n",
    "    documents.append(Document(page_content=text, metadata={\"source\": url}))\n",
    "\n",
    "print(f\"Created {len(documents)} LangChain documents\")\n",
    "if documents:\n",
    "    print(f\"Sample preview: {documents[0].page_content[:200]}...\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "375353ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split documents into list of chunks for better retrieval\n",
    "\n",
    "splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "all_chunks = splitter.split_documents(documents)\n",
    "\n",
    "print(f\"Created {len(all_chunks)} chunks from {len(documents)} documents\")\n",
    "if all_chunks:\n",
    "    print(f\"Sample preview: {all_chunks[0].page_content[:200]}...\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5f7a191",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Pinecone (for vector storage and retrieval)\n",
    "\n",
    "INDEX_NAME = \"berkeley-course-rag\"\n",
    "\n",
    "pinecone = Pinecone(api_key=os.environ[\"PINECONE_API_KEY\"])\n",
    "\n",
    "RESET_INDEX = True  # will always rebuild index (can change to False to avoid this)\n",
    "if RESET_INDEX and INDEX_NAME in pinecone.list_indexes().names():\n",
    "    pinecone.delete_index(INDEX_NAME)\n",
    "\n",
    "# Create Pinecone index\n",
    "\n",
    "index = pinecone.create_index_for_model(\n",
    "        name=INDEX_NAME,\n",
    "        cloud=\"aws\",\n",
    "        region=\"us-east-1\",\n",
    "        embed={\n",
    "            \"model\":\"llama-text-embed-v2\",\n",
    "            \"field_map\":{\"text\": \"chunk_text\"}\n",
    "        }\n",
    "    )\n",
    "\n",
    "print(f\"Creating Pinecone index: {INDEX_NAME}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0edcf1e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop through chunks, create embeddings, and upsert to Pinecone\n",
    "\n",
    "index = pinecone.Index(INDEX_NAME)\n",
    "\n",
    "#Preparing records with unique IDs (source URL + chunk index)\n",
    "records = []\n",
    "for i, chunk in enumerate(all_chunks):\n",
    "    chunk_id = str(i)\n",
    "    records.append({\n",
    "        \"id\": chunk_id,\n",
    "        \"chunk_text\": chunk.page_content,\n",
    "        \"source\": chunk.metadata['source']\n",
    "    })\n",
    "\n",
    "#Upsert to Pinecone in batches\n",
    "BATCH_SIZE = 96 #Pinecone limit is 96 records per batch\n",
    "for i in range(0, len(records), BATCH_SIZE):\n",
    "    batch = records[i:i + BATCH_SIZE]\n",
    "    index.upsert_records(\"test-namespace\", batch)\n",
    "    print(f\"Upserted batch {i//BATCH_SIZE + 1} ({len(batch)} records)\")\n",
    "print(f\"All {len(records)} chunks embedded and stored in Pinecone!\")\n",
    "    \n",
    "print(f\"Index stats: {index.describe_index_stats()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51b5e630",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Gemini LLM (for answer generation)\n",
    "\n",
    "client = genai.Client()\n",
    "\n",
    "response = client.models.generate_content(\n",
    "    model=\"gemini-2.0-flash-exp\", contents=\"Explain how AI works in a few words\"\n",
    ")\n",
    "print(response.text)\n",
    "\n",
    "print(\"Gemini LLM initialized\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1856522",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build RAG query function\n",
    "def ask_question(question, index, k=3):\n",
    "    \"\"\"\n",
    "    Query Pinecone for relevant chunks and generate answer with Gemini.\n",
    "    \n",
    "    Args:\n",
    "        question: User's question\n",
    "        index: Pinecone index\n",
    "        embeddings: Embedding model\n",
    "        k: Number of chunks to retrieve\n",
    "    Returns:\n",
    "        (answer, sources): Generated answer and source URLs\n",
    "    \"\"\"\n",
    "    \n",
    "    #Query Pinecone with the question\n",
    "    results = index.search(\n",
    "        namespace=\"test-namespace\",\n",
    "        query={\n",
    "            \"inputs\": {\"text\": question},\n",
    "            \"top_k\": k\n",
    "        },\n",
    "        fields=[\"chunk_text\", \"source\"]\n",
    "    )\n",
    "\n",
    "    #Extracting the relevant chunks and sources\n",
    "    retrieved_chunks = []\n",
    "    sources = []\n",
    "    if 'result' in results and 'hits' in results['result']:\n",
    "        for hit in results['result']['hits']:\n",
    "            chunk_text = hit.get('fields', {}).get('chunk_text', '')\n",
    "            source = hit.get('fields', {}).get('source', 'Unknown')\n",
    "            if chunk_text:\n",
    "                retrieved_chunks.append(chunk_text)\n",
    "                if source not in sources:\n",
    "                    sources.append(source)\n",
    "    \n",
    "    #Preparing context for the LLM\n",
    "    context = \"\\n\\n\".join([f\"Chunk {i+1}:\\n{chunk}\" for i, chunk in enumerate(retrieved_chunks)])\n",
    "\n",
    "    #Generating sn answer through Gemini and extracting sources\n",
    "    prompt = f\"\"\"Answer the following question based on the provided context from the CS 61B course materials.\n",
    "        Question: {question}\n",
    "        Context from course materials: {context}\n",
    "        Please provide a comprehensive answer based on the context above. If the context doesn't contain relevant information, say so.\"\"\"\n",
    "    response = client.models.generate_content(\n",
    "        model=\"gemini-2.0-flash-exp\",\n",
    "        contents=prompt\n",
    "    )\n",
    "    return response.text, sources\n",
    "answer, sources = ask_question(\"What is the exam policy for CS 61B?\", index, k=3)\n",
    "print(f\"Answer: {answer}\\n\\nSources: {sources}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (jcp)",
   "language": "python",
   "name": "jcp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
